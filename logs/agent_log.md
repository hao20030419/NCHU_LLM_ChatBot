# Agent 開發對話紀錄

## 2025-12-01 啟動專題
使用 ChatGPT 設計 Streamlit Chatbot 基礎架構。

【對話摘要】
- 決定專題為「文字生成聊天機器人」
- 建立 GitHub 專案結構
- 完成最小可執行 Streamlit chat 範例

## 2025-12-01 接入語言模型（GPT-2）

已完成：
- 安裝 transformers / torch
- 將假回應改為 HuggingFace GPT-2 模型
- 成功實現即時文字生成

問題：
- 初次載入模型較慢（需下載）
- GPT-2 中文能力有限，後續可改為其他模型

成果：
- 使用者輸入文字可即時生成回答

## 2025-12-01 更換模型為 Qwen2.5-1.5B

完成事項：
- 將原本 GPT2 改為 Qwen2.5-1.5B-Instruct
- 成功進行中文對話
- 建立上下文輸入拼接
- 模型可在本機 CPU 運作

觀察：
- 初次下載模型時間較久
- 中文生成效果明顯優於 GPT2

結論：
- 選擇 Qwen2.5-1.5B 作為專題模型

## 2025-12-01 新增角色與互動功能
- 加入 system prompt 設計
- 增加重設對話功能
- 提升對話一致性與教學風格

## 2025-12-01 更換模型為 Qwen2.5-0.5B

### 問題背景
原先嘗試於 Streamlit Cloud 部署 Qwen2.5-1.5B 模型時，因模型檔案體積較大，導致在免費雲端環境中發生記憶體不足與啟動失敗問題。

### 處理方式
經分析部署限制後，改採 Qwen2.5-0.5B-Instruct 作為替代模型，並進行以下調整：

- 更換模型名稱為 Qwen/Qwen2.5-0.5B-Instruct  
- 修正參數 `torch_dtype` 為 `dtype`，避免 API 警告  
- 重新 push 至 GitHub 觸發自動部署  
- 保留系統 prompt 與對話記憶機制

### 成果
- 成功於 Streamlit Cloud 完成部署  
- 模型可即時回覆中文內容  
- 回應品質穩定，適用於課程展示  
- 部署時間顯著縮短

### 結論
在雲端資源有限情境下，選擇較小尺寸之模型可有效降低部署門檻，同時維持可用之語言理解與生成能力。本次調整使專題順利完成部署，並符合展示與作業繳交需求。

## 2025-12-01 改用超小模型以完成雲端部署

### 問題背景
原先嘗試於 Streamlit Cloud 部署 Qwen2.5 系列模型時，因免費環境記憶體限制，模型於載入階段即被系統終止，導致應用程式無法正常啟動。

### 處理方式
為確保成果可成功展示與繳交，決定改用體積小、資源需求低之語言模型 distilgpt2，並同步簡化模型載入流程，避免記憶體溢位問題。

調整項目如下：
- 將模型更換為 distilgpt2
- 移除 accelerate 與 device_map 設定
- 採用 HuggingFace pipeline 結構
- 保留系統提示語（System Prompt）
- 維持多輪對話紀錄機制

### 成果
- Streamlit Cloud 成功完成部署
- 網頁可即時生成回應
- 系統穩定啟動，無記憶體錯誤
- 達成線上 Demo 與互動展示需求

### 結論
在資源受限環境中，模型尺寸為影響部署可行性的關鍵因素。本次改用超小模型使專題順利完成雲端展示，亦成為實務上「模型效能與系統資源取捨」的重要案例。